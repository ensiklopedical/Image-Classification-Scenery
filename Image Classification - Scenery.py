# -*- coding: utf-8 -*-
"""Image Classification Model Deployment - Faisal Ahmad Gifari - Dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_1HnJH6veGc6NAtD3NWtZ2DClX6K1ndW

# Data Diri

- Nama : Faisal Ahmad Gifari
- Jenis Kelamin : Laki-Laki
- Pekerjaaan : Mahasiswa
- Tempat/Tanggal Lahir : Kuningan, 17 September 2002
- Username : faisal_ag_037
- email : pd-20379543@edu.jakarta.go.id
- No. Telepon : 085775063559
- Kota Domisili : Jakarta Barat
- Institusi : UIN Syarif Hidayatullah Jakarta

# Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import os
import cv2
import random
from PIL import Image
import tensorflow as tf
import tensorflow.keras
import zipfile
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
from PIL import Image
from tensorflow.keras.layers import BatchNormalization
import math
import shutil
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import Callback
from keras.callbacks import EarlyStopping
from google.colab import files
from keras.preprocessing import image
# %matplotlib inline

"""# Importing Dataset"""

from google.colab import drive
drive.mount('/content/gdrive')
!cp '/content/gdrive/MyDrive/Dataset/intel_image.zip' intel_image.zip

"""=== Dataset ===

Dataset yang digunakan adalah dataset yang saya gabungkan secara manual dari dua sumber berbeda. Kemudian keduanya tidak pernah saya gunakan untuk submission machine learning sebelumnya, berikut adalah sumbernya:

1. Intel Image Classification
-> https://www.kaggle.com/datasets/puneet6060/intel-image-classification/data

2. Landscape Recognition
-> https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images

Hanya diambil 3 kelas atau kategori pemandangan, yaitu:
- Mountain
- Forest
- Glacier

Selain itu, jumlah data dari tiap kelasnya diseimbangkan jumlahnya menjadi 4000 tiap kelasnya. Maka dari itu, total gambar pada dataset ini bertotal 12000

===Kesimpulan terkait dataset===

Dapat dikatan, bahwa dataset yang saya gunakan:
- Belum pernah digunakan pada submission kelas machine learning sebelumnya.
- Dataset yang digunakan memiliki lebih dari 10000 gambar.

Berikut adalah pembuktiannya:
"""

zip_ref = zipfile.ZipFile('/content/intel_image.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

folder = '/content/intel_image'
list_dir = []

for root, dir, pics in os.walk(folder):
  for file in pics:
    file_path = os.path.join(root, file)
    list_dir.append(file_path)


total_dataset = len(list_dir)
print('Total gambar pada dataset ini sebanyak :', total_dataset, "gambar")

"""Terbukti, dataset yang digunakan berjumlah 12000 gambar. Maka dari itu, dataset sudah memenuhi syarat **minimal 10000 gambar** untuk skor bintang 5"""

list_res = set()

for file_path in list_dir:
  img = cv2.imread(file_path)
  if img is not None:
    res = img.shape
    list_res.add(res)

total_res = len(list_res)
print('Total variasi resolusi gambar pada dataset :', total_res)

"""Terbukti, dataset yang digunakan memiliki berbagai resolusi gambar. Maka dari itu, dataset sudah memenuhi syarat **resolusi gambar pada dataset tidak seragam** untuk skor bintang 5"""

#Pembagian dataset menjadi Train Data dan Val Data
file_path = '/content/intel_image'
objects = ['forest', 'glacier', 'mountain']

for i in objects:
  os.makedirs(file_path + '/train/' + i)
  os.makedirs(file_path + '/val/' + i)

  all_pictures = os.listdir(file_path + '/' + i)
  np.random.shuffle(all_pictures)

  val_rasio = 0.2
  train_pictures, val_pictures = np.split(np.array(all_pictures),
                                          [math.ceil(float(
                                              (len(all_pictures)*(1 - val_rasio))))])

  train_pictures = [file_path + '/' + i + '/' + pict for pict in train_pictures.tolist()]
  val_pictures = [file_path + '/' + i + '/' + pict for pict in val_pictures.tolist()]

  for pict in train_pictures:
    shutil.copy(pict, file_path + '/train/' + i)
  for pict in val_pictures:
    shutil.copy(pict, file_path + '/val/' + i)

"""Dataset sudah dibagi menjadi **80% train set** dan **20% test set**


```
# val_rasio = 0.2
```






"""

print(os.listdir('/content/intel_image/train'))
print(os.listdir('/content/intel_image/val'))

"""Data train dan Data test sudah dibagi. berikut adalah perhitungannya:"""

def count_images_in_folder(folder_path):
    image_extensions = '.png'
    image_count = 0

    for filename in os.listdir(folder_path):
        if any(filename.endswith(ext) for ext in image_extensions):
            try:
                img = Image.open(os.path.join(folder_path, filename)) # open the image file
                img.verify() # verify that it is, in fact an image
                image_count += 1
            except (IOError, SyntaxError) as e:
                print('Bad file:', filename) # print out the names of corrupt files

    return image_count

#Train Data
train_mountain = '/content/intel_image/train/mountain'
train_forest = '/content/intel_image/train/forest'
train_glacier = '/content/intel_image/train/glacier'


print(f'Ada {count_images_in_folder(train_mountain)} images Mountain untuk train data.')
print(f'Ada {count_images_in_folder(train_forest)} images Forest untuk train data.')
print(f'Ada {count_images_in_folder(train_glacier)} images Glacier untuk train data.')
print("=========================================")
train_count = 0
for Obj in objects:
  path_count = '/content/intel_image/train/'
  train_count += count_images_in_folder(path_count + Obj)
print(f'Total ada {train_count} images untuk train data.')

train_folders = ['/content/intel_image/train/mountain',
           '/content/intel_image/train/forest',
           '/content/intel_image/train/glacier']

for folder in train_folders:
    # Get a list of all the image files in the folder
    images = os.listdir(folder)

    # Select a random image file
    selected_image = random.choice(images)

    # Create the full image path
    image_path = os.path.join(folder, selected_image)

    # Open and display the image
    image = Image.open(image_path)
    plt.imshow(image)
    plt.title(folder)
    plt.show()

"""Terbukti, **resolusi gambar pada dataset tidak seragam**"""

#Val Data
val_mountain = '/content/intel_image/val/mountain'
val_forest = '/content/intel_image/val/forest'
val_glacier = '/content/intel_image/val/glacier'


print(f'Ada {count_images_in_folder(val_mountain)} images Mountain untuk val data.')
print(f'Ada {count_images_in_folder(val_forest)} images Forest untuk val data.')
print(f'Ada {count_images_in_folder(val_glacier)} images Glacier untuk val data.')
print("=========================================")
val_count = 0
for Obj in objects:
  path_count = '/content/intel_image/val/'
  val_count += count_images_in_folder(path_count + Obj)
print(f'Total ada {val_count} images untuk train data.')

val_folders = ['/content/intel_image/val/mountain',
               '/content/intel_image/val/forest',
               '/content/intel_image/val/glacier']

for folder in val_folders:
    # Get a list of all the image files in the folder
    images = os.listdir(folder)

    # Select a random image file
    selected_image = random.choice(images)

    # Create the full image path
    image_path = os.path.join(folder, selected_image)

    # Open and display the image
    image = Image.open(image_path)
    plt.imshow(image)
    plt.title(folder)
    plt.show()

total_image = train_count + val_count
print(f'Total ada {total_image} images pada dataset ini')

if (val_count/total_image) == val_rasio:
  print("Rasio sudah sesuai, yaitu sebesar 20%")
else:
  print("Rasio belum sesuai, belom sebesar 20%")

"""# Modelling"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range = 0.00001,
    zoom_range=0.01,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale = 1./255)

train_batch_size = 64
val_batch_size = 32

train_generator = train_datagen.flow_from_directory('/content/intel_image/train',
                                                    target_size = (150, 150),
                                                    batch_size = train_batch_size, # 13, 26, 43 // 172, 86, 52
                                                    shuffle = True,
                                                    class_mode = 'categorical')

validation_generator = test_datagen.flow_from_directory('/content/intel_image/val',
                                                        target_size = (150, 150),
                                                        batch_size = val_batch_size, # 6, 9 , 18 // 93, 62, 31
                                                        class_mode = 'categorical')

Model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3, 3), use_bias = True, padding = 'same', input_shape = (150, 150, 3)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Conv2D(128, (3, 3), use_bias = True, padding = 'same', strides=1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Conv2D(256, (3, 3), use_bias = True, padding = 'same', strides=1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.Conv2D(512, (3, 3), use_bias = True, padding = 'same', strides=1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1024, activation = 'relu'),
    tf.keras.layers.Dense(512, activation = 'relu'),
    tf.keras.layers.Dense(3, activation = 'softmax',)
])

"""Terbukti, bahwa model menggunakan **model sequential**

Terbukti, bahwa model menggunakan **Conv2D Maxpooling Layer**
"""

Model.summary()

class EarlyStopper(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92 and logs.get('val_accuracy')>0.92):
      print("STOP || Model has reach 92% accuracy and and val accuracy")
      self.model.stop_training = True

EarlyStopper1= EarlyStopper()

"""Terbukti, saya menggunakan **Callback** untuk memberhentikan proses training jika sudah mencapai batas accuracy dan val_accuracy yang sudah ditentukan."""

class TrainingPlot(keras.callbacks.Callback):

    # This function is called when the training begins
    def on_train_begin(self, logs={}):
        # Initialize the lists for holding the logs, losses and accuracies
        self.losses = []
        self.acc = []
        self.val_losses = []
        self.val_acc = []
        self.logs = []

    # This function is called at the end of each epoch
    def on_epoch_end(self, epoch, logs={}):

        # Append the logs, losses and accuracies to the lists
        self.logs.append(logs)
        self.losses.append(logs.get('loss'))
        self.acc.append(logs.get('accuracy'))
        self.val_losses.append(logs.get('val_loss'))
        self.val_acc.append(logs.get('val_accuracy'))

        # Before plotting ensure at least 2 epochs have passed
        if len(self.losses) > 1:

            N = np.arange(0, len(self.losses))

            plt.figure()
            plt.plot(N, self.losses, label = "train_loss")
            plt.plot(N, self.acc, label = "train_acc")
            plt.plot(N, self.val_losses, label = "val_loss")
            plt.plot(N, self.val_acc, label = "val_acc")
            plt.title("Training Loss and Accuracy [Epoch {}]".format(epoch))
            plt.xlabel("Epoch #")
            plt.ylabel("Loss/Accuracy")
            plt.legend()
            plt.show()
            plt.close()

TrainingPlot1 = TrainingPlot()

"""Terbukti, saya menggunakan **plot** untuk memantau progress akurasi dan loss model saat proses training berlangsung"""

Optimizer=tf.optimizers.RMSprop(learning_rate=0.0001, momentum = 0.9)

Model.compile(loss = 'categorical_crossentropy',
              optimizer = Optimizer,
              metrics = ['accuracy'])

Model.fit(train_generator,
          epochs = 150,
          steps_per_epoch = train_count // train_batch_size,
          validation_data = validation_generator,
          validation_steps = val_count // val_batch_size,
          callbacks = [EarlyStopper1, TrainingPlot1],
          verbose = 2)

exp_folder = 'saved-model/'
tf.saved_model.save(Model, exp_folder)

SM_converter = tf.lite.TFLiteConverter.from_saved_model('saved-model/')
SM_tflite_model = SM_converter.convert()

with tf.io.gfile.GFile('SM-Mountain-Forest-Glacier_Classification.tflite', 'wb') as f:
    f.write(SM_tflite_model)

"""==== Kesimpulan ====

Seluruh proses berhasil dijalankan menggunakan Dataset yang **belum pernah digunakan pada submission kelas machine learning sebelumnya**. Selain itu, dataset yang digunakan berjumlah **12000 gambar** (lebih besar dari 10000 gambar) dari gabungan dua dataset berbeda. Kemudian, dataset tersebut **memiliki resolusi yang tidak seragam** atau berbeda beda. Selanjutnya, **Dataset juga dibagi menjadi 80% train dan 20% train set.**


Dataset ini digunakan untuk membangun model klasifikasi pemandangan dengan **3 kelas atau label berbeda**, yaitu **Mountain, Forest, dan Glacier**


Pada proses training, saya menggunakan **model sequential** yang memiliki layers **Conv2D Maxpooling Layer**. Kemudian, saya juga menambahkan 2 **Callback**, yang pertama adalah **EarlyStopper** dan yang kedua adalah **Plot Training**.


Hasil dari proses training memberikan hasil yang **sangat bagus**, yaitu **accuracy** pada training set sebesar **94,6%** dan accuracy pada validation set (**val_accuracy**) sebesar **92,5%**.


Terakhir, model disimpan dalam bentuk atau **format SavedModel** **dikonversi** menjadi format **.tflite** yang berjudul
**"SM-Mountain-Forest-Glacier_Classification.tflite"**
"""